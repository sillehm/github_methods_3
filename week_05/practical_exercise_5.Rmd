---
title: "practical_exercise_5, Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: '[FILL IN THE DATE]'
output:
  html_document:
    df_print: paged
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readbulk, lme4, stats, boot, multcomp)
```

# Exercises and objectives
The objectives of the exercises of this assignment are based on: https://doi.org/10.1016/j.concog.2019.03.007  
  
4) Download and organise the data from experiment 1  
5) Use log-likelihood ratio tests to evaluate logistic regression models  
6) Test linear hypotheses  
7) Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is part 2 of Assignment 2 and will be part of your final portfolio


# EXERCISE 4 - Download and organise the data from experiment 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 1 (there should be 29).  
The data is associated with Experiment 1 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  
  
1) Put the data from all subjects into a single data frame - note that some of the subjects do not have the _seed_ variable. For these subjects, add this variable and make in _NA_ for all observations. (The _seed_ variable will not be part of the analysis and is not an experimental variable)
```{r message=TRUE, warning=TRUE, 4.1}
df <- read_bulk(directory = "/Users/Sillemarkussen/Desktop/github_methods_3/week_05/experiment_1", stringsAsFactors = T)
```
     i. Factorise the variables that need factorising  
```{r 4.1.i}
df$subject <- as.factor(df$subject)
df$pas <- as.factor(df$pas)

```
     
    ii. Remove the practice trials from the dataset (see the _trial.type_ variable)
```{r 4.1.ii}
df <- df %>% 
  filter(trial.type=="experiment")
```
  
    iii. Create a _correct_ variable  
```{r 4.1.iii}
df$target.type <- as.character(df$target.type)

df$correct <- ifelse((df$target.type == "even" & df$obj.resp == "e"| df$target.type == "odd" & df$obj.resp == "o" ), 1, 0)
```
    
    iv. Describe how the _target.contrast_ and _target.frames_ variables differ compared to the data from part 1 of this assignment  
In the second experiment the target contrast was adjusted for each participant. Contrary, the first experiment had a target contrast value set at 0.1 for all participants. 
The variable target frames indicates the amount of frames the target object is presented in. One frame is 11.8 ms. 

# EXERCISE 5 - Use log-likelihood ratio tests to evaluate logistic regression models

1) Do logistic regression - _correct_ as the dependent variable and _target.frames_ as the independent variable. (Make sure that you understand what _target.frames_ encode). Create two models - a pooled model and a partial-pooling model. The partial-pooling model should include a subject-specific intercept. 
```{r message=FALSE, warning=FALSE, 5.1}
pool_m <- glm(correct ~ target.frames, df, family = "binomial")
partpool_m <- glmer(correct ~ target.frames + (1 | subject), df, family = "binomial")
```

    i. the likelihood-function for logistic regression is: $L(p)={\displaystyle\prod_{i=1}^Np^{y_i}(1-p)^{(1-y_i)}}$ (Remember the probability mass function for the Bernoulli Distribution). Create a function that calculates the likelihood. 
```{r 5.1.i}
likelihood_fun <- function(model){
  yhat <- fitted.values(model)
  y <- df$correct
  prod(yhat^(y)*(1-yhat)^(1-y))
}
```
    
    ii. the log-likelihood-function for logistic regression is: $l(p) = {\displaystyle\sum_{i=1}^N}[y_i\ln{p}+(1-y_i)\ln{(1-p)}$. Create a function that calculates the log-likelihood  
```{r 5.1.ii}
likelihood_log_fun <- function(model){
  yhat <- fitted.values(model)
  y <- df$correct
  sum(y*log(yhat)+(1-y)*log(1-yhat))
}
```
    
    iii. apply both functions to the pooling model you just created. Make sure that the log-likelihood matches what is returned from the _logLik_ function for the pooled model. Does the likelihood-function return a value that is surprising? Why is the log-likelihood preferable when working with computers with limited precision?
```{r 5.1.iii}
likelihood_fun(pool_m)
likelihood_log_fun(pool_m)

exp(logLik(pool_m))
logLik(pool_m)
```
The log-likelihood function matches with the return of the logLik function. 
The likelihood function returns 0 as does the exponential of the logLik function. This is surprising since, in theory, this value is not possible, since there cannot be a likelihood of 0 of observing something that has already been observed. Also, for the function to return 0 either y or yhat has to be zero. It is not possible for yhat to be zero since it is on the logarithmic scale and the logarithm of 0 is not defined. However, in reality it is not returning 0, it is returning a very small likelihood that is not precisely estimated by the computer. Therefore, it makes sense to use log-likelihood since our computers can actually estimate these numbers more precisely, which makes it easier for us to contrast them.

    iv. now show that the log-likelihood is a little off when applied to the partial pooling model - (the likelihood function is different for the multilevel function - see section 2.1 of https://www.researchgate.net/profile/Douglas-Bates/publication/2753537_Computational_Methods_for_Multilevel_Modelling/links/00b4953b4108d73427000000/Computational-Methods-for-Multilevel-Modelling.pdf if you are interested)  
```{r 5.1.iv}
likelihood_log_fun(partpool_m)
logLik(partpool_m)
```
As can be seen from the output the log-likelihood function is a little off. Since the likelihood function is different for the multilevel function the function returns a wrong log-likelihood. 

2) Use log-likelihood ratio tests to argue for the addition of predictor variables, start from the null model, `glm(correct ~ 1, 'binomial', data)`, then add subject-level intercepts, then add a group-level effect of _target.frames_ and finally add subject-level slopes for _target.frames_. Also assess whether or not a correlation between the subject-level slopes and the subject-level intercepts should be included.
```{r 5.2}
m1 <- glm(correct ~ 1, 'binomial', df)
m2 <- glmer(correct ~ 1 + (1 | subject), df, "binomial")
m3 <- glmer(correct ~ target.frames + (1 | subject), df, "binomial")
m4 <- glmer(correct ~ target.frames + (1 + target.frames || subject), df, "binomial")
m5 <- glmer(correct ~ target.frames + (1 + target.frames | subject), df, "binomial")

anova(m5, m1, m2, m3, m4)


```

The underlying null hypothesis of the anova method is that the addition of parameters between the models are just adding noise. 
We can see that the 5th model (correct ~ target.frames + (1 + target.frames | subject)) has the highest log-likelihood and a significant p-value. This signifies that the reduction in deviance would be surprising if the added parameters are just noise. The model including a correlation between the subject-level slopes and the subject-level intercepts has a lower log-likelihood than the similar model not including a correlation.

    i. write a short methods section and a results section where you indicate which model you chose and the statistics relevant for that choice. Include a plot of the estimated group-level function with `xlim=c(0, 8)` that includes the estimated subject-specific functions.
```{r 5.2.i, message=FALSE, warning=FALSE}
df %>% ggplot() +
  geom_smooth(aes(x = target.frames, y = fitted.values(pool_m), color= "Complete Pooling")) +
  geom_smooth(aes(x = target.frames, y = fitted.values(m5), color= "Partial Pooling")) + 
  facet_wrap(~ subject) +
  xlab("Target Frames") + 
  ylab("Fitted Values") +
  xlim(c(0,6))+
  ylim(c(0,1))+
  ggtitle('Estimated Subject-Specific Functions')+
  theme_minimal()
```
 <br>
 A null model was constructed followed by models of increasing complexity including multi level effects such as random slope and intercept. Using the anova function, the null model was compared to the more complex models.
 
 The chosen model predicts correct from target frames and include target frames as random intercept and subject as random slopes. 
 _correct ~ target.frames + (1 + target.frames | subject)_
 
 The model was significant, with a logLIk = -10449, deviance = 20898 and \chi(1) = 22.926 and p < 0.5.
 
 This was chosen since it has the highest loglik value, lowest deviance and has a significant p-value, and thus seemed the better choice compared to the other, more simpler models, since the added complexity did not seem to be pure noise. 
  
    ii. also include in the results section whether the fit didn't look good for any of the subjects. If so, identify those subjects in the report, and judge (no statistical test) whether their performance (accuracy) differed from that of the other subjects. Was their performance better than chance? (Use a statistical test this time) (50 %)  
 Subject 24 looks a little odd when looking at the partial pooling function of that subject. The deviance between the complete pooling  and the partial pooling for that subject suggest that the performance differed from the average of all the subjects.
 
```{r 5.2.ii}
# Testing the performance of subject 24 statistically 
subject24 <- df %>% filter(subject == "24") 

t.test(subject24$correct, mu=0.5)
```
Using a one sample t-test 
56% correct skriv mere *** significantly better than chance 

3) Now add _pas_ to the group-level effects - if a log-likelihood ratio test justifies this. Also add the interaction between _pas_ and _target.frames_ and check whether a log-likelihood ratio test justifies this  
```{r 5.3}
pas_m <- glmer(correct ~ target.frames + pas + (1 + target.frames | subject), df, "binomial")
pas_m_int <- glmer(correct ~ target.frames * pas + (1 + target.frames | subject), df, "binomial")

anova(pas_m, pas_m_int, m5)
```
A log-likelihood ratio test justifies the addition of the interaction between _pas_ and _target.frames_ to the group-level effects since the p-value is significant.

    ii. plot the estimated group-level functions over `xlim=c(0, 8)` for each of the four PAS-ratings - add this plot to your report (see: 5.2.i) and add a description of your chosen model. Describe how _pas_ affects accuracy together with target duration if at all. Also comment on the estimated functions' behaviour at target.frame=0 - is that behaviour reasonable?  
```{r 5.3.ii}
# model with group level effects
pas_m_pool <- glm(correct ~ target.frames * pas, df, family="binomial")

# plotting
df %>% ggplot(aes(x = target.frames, y = fitted.values(pas_m_pool), color= pas)) +
  geom_line()+
  xlab("Target Frames") + 
  ylab("Fitted Values") +
  xlim(c(0,6))+
  ylim(c(0,1))+
  labs(title = "Estimated group-level functions for each PAS-rating")+
  theme_minimal()


```

```{r}
# Extracting estimates
summary(pas_m_int)$coefficients
```

The chosen model 
_correct ~ target.frames * pas + (1 + target.frames | subject)_
includes an interaction effect and it is expected that the duration of target frames affects accuracy differently with different pas ratings. 
It is justified by the result of the anova and by visually inspecting the plot, that the interaction between pas and target frames affects accuracy.
Specifically, the estimate of target.frames:pas2 is lower than the estimate of target.frames:pas3 meaning that an increase in target frames is affecting accuracy more in pas 3 than the same increase in target frames will affect accuracy in pas 2. All but one interaction effects (frames:pas2) are significant, suggesting this difference in the influence of the increase of target frames across the four pas levels. 

At a target level of 0 the performance is expected to be at chance level. The intercept of the model is predicted to be 46.96% which is somewhat close to chance as expected.


# EXERCISE 6 - Test linear hypotheses

In this section we are going to test different hypotheses. We assume that we have already proved that more objective evidence (longer duration of stimuli) is sufficient to increase accuracy in and of itself and that more subjective evidence (higher PAS ratings) is also sufficient to increase accuracy in and of itself.  
We want to test a hypothesis for each of the three neighbouring differences in PAS, i.e. the difference between 2 and 1, the difference between 3 and 2 and the difference between 4 and 3. More specifically, we want to test the hypothesis that accuracy increases faster with objective evidence if subjective evidence is higher at the same time, i.e. we want to test for an interaction.  

1) Fit a model based on the following formula: `correct ~ pas * target.frames + (target.frames | subject))`

    i. First, use `summary` (yes, you are allowed to!) to argue that accuracy increases faster with objective evidence for PAS 2 than for PAS 1. 
```{r 6.1.i}
summary(pas_m_int)
```
  The coefficient for the interaction effect of pas 2 is positive (specifically 0.44). This signifies that accuracy increases faster with objective evidence for PAS 2 than for PAS 1.
  
2) `summary` won't allow you to test whether accuracy increases faster with objective evidence for PAS 3 than for PAS 2 (unless you use `relevel`, which you are not allowed to in this exercise). Instead, we'll be using the function `glht` from the `multcomp` package

    i. To redo the test in 6.1.i, you can create a _contrast_ vector. This vector will have the length of the number of estimated group-level effects and any specific contrast you can think of can be specified using this. For redoing the test from 6.1.i, the code snippet below will do
```{r 6.2.i}
## testing whether PAS 2 is different from PAS 1
contrast.vector1 <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(pas_m_int, contrast.vector1)
print(summary(gh))
```
    
    ii. Now test the hypothesis that accuracy increases faster with objective evidence for PAS 3 than for PAS 2.
```{r 6.2.ii}
## as another example, we could also test whether there is a difference in
## intercepts between PAS 2 and PAS 3
contrast.vector2 <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1)
gh <- glht(pas_m_int, contrast.vector2)
print(summary(gh))
```
  The estimate is positive which signifies the faster increase of accuracy with objective evidence for PAS 3 than for PAS 2.
  
    iii. Also test the hypothesis that accuracy increases faster with objective evidence for PAS 4 than for PAS 3
```{r 6.2.iii}
contrast.vector3 <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)
gh3 <- glht(pas_m_int, contrast.vector3)
print(summary(gh3))
```
   The estimate is positive, however, it is a low and not significant estimate. Thus, not suggesting significantly faster increases with objective evidence for PAS 4 than for PAS 3.
  
3) Finally, test that whether the difference between PAS 2 and 1 (tested in 6.1.i) is greater than the difference between PAS 4 and 3 (tested in 6.2.iii)
```{r 6.3}
# looking at the difference of differences
# difference of differences
K <- contrast.vector1 - contrast.vector3


t <- glht(pas_m_int, linfct = K)
summary(t)
```
There is a significant difference between the increase in accuracy between PAS 2 and 1 and the increase of accuracy between PAS 4 and 3. Since the estimate is positive, the difference between PAS 2 and 1 is greater than the difference between PAS 4 and 3.

### Snippet for 6.2.i
```{r}
## testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(pas_m_int, contrast.vector)
print(summary(gh))
## as another example, we could also test whether there is a difference in
## intercepts between PAS 2 and PAS 3
contrast.vector <- matrix(c(0, -1, 1, 0, 0, 0, 0, 0), nrow=1)
gh <- glht(pas_m_int, contrast.vector)
print(summary(gh))
```

# EXERCISE 7 - Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

We saw in 5.3 that the estimated functions went below chance at a target duration of 0 frames (0 ms). This does not seem reasonable, so we will be trying a different approach for fitting here.  
We will fit the following function that results in a sigmoid, $f(x) = a + \frac {b - a} {1 + e^{\frac {c-x} {d}}}$  
It has four parameters: _a_, which can be interpreted as the minimum accuracy level, _b_, which can be interpreted as the maximum accuracy level, _c_, which can be interpreted as the so-called inflexion point, i.e. where the derivative of the sigmoid reaches its maximum and _d_, which can be interpreted as the steepness at the inflexion point. (When _d_ goes towards infinity, the slope goes towards a straight line, and when it goes towards 0, the slope goes towards a step function).  
  
We can define a function of a residual sum of squares as below

```{r}
RSS <- function(dataset, par)
{
    ## "dataset" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    a = par[1]
    b = par[2]
    c = par[3]
    d = par[4]
    
    x <- dataset$x
    y <- dataset$y
    
    y.hat <- a + ((b-a)/(1 + exp((c-x)/d)))
    
    RSS <- sum((y - y.hat)^2)
    return(RSS)
}
```

1) Now, we will fit the sigmoid for the four PAS ratings for Subject 7
    i. use the function `optim`. It returns a list that among other things contains the four estimated parameters. You should set the following arguments:  
    `par`: you can set _c_ and _d_ as 1. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `fn`: which function to minimise?  
    `data`: the data frame with _x_, _target.frames_, and _y_, _correct_ in it  
    `method`: 'L-BFGS-B'  
    `lower`: lower bounds for the four parameters, (the lowest value they can take), you can set _c_ and _d_ as `-Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `upper`: upper bounds for the four parameters, (the highest value they can take) can set _c_ and _d_ as `Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
```{r 7.1.i}
subject7 <- df %>%
  dplyr::filter(subject == '7') %>% 
  dplyr::select('x' = target.frames, 'y' = correct, pas)


par1 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '1'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par2 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '2'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par3 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '3'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par4 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '4'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

```

A is set to 0.5 since the expected minimum in accuracy is chance level. 
B is set to 1 since it is not possible to be more than 100% accurate. 
The minimum and the maximum accuracy level has a lower bound at 0.5 and an upper bound at 1 since the possible accuracy level has to be whithin this range. 

    ii. Plot the fits for the PAS ratings on a single plot (for subject 7) `xlim=c(0, 8)`
```{r 7.1.ii}
# New data frame with hypothetical x values 
newdf <- data.frame(cbind('x' = seq(0, 8, by = 0.01)))

# Calculating the yhats using the sigmoid function
newdf$yhat1 <- par1$par[1] + ((par1$par[2]-par1$par[1])/(1 + exp((par1$par[3]-newdf$x)/par1$par[4])))
newdf$yhat2 <- par2$par[1] + ((par2$par[2]-par2$par[1])/(1 + exp((par2$par[3]-newdf$x)/par2$par[4])))
newdf$yhat3 <- par3$par[1] + ((par3$par[2]-par3$par[1])/(1 + exp((par3$par[3]-newdf$x)/par3$par[4])))
newdf$yhat4 <- par4$par[1] + ((par4$par[2]-par4$par[1])/(1 + exp((par4$par[3]-newdf$x)/par4$par[4])))

# Plotting
ggplot(newdf) + 
  geom_line(aes(x = x, y = yhat1, color = 'Pas 1')) + 
  geom_line(aes(x = x, y = yhat2, color = 'Pas 2')) + 
  geom_line(aes(x = x, y = yhat3, color = 'Pas 3')) + 
  geom_line(aes(x = x, y = yhat4, color = 'Pas 4')) + 
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  ggtitle('PAS Ratings Sigmoid Fits for Participant 7')+
  theme_minimal()

```

    iii. Create a similar plot for the PAS ratings on a single plot (for subject 7), but this time based on the model from 6.1 `xlim=c(0, 8)` 
```{r 7.1.iii}
# New data frame with hypothetical values for target frames between 0 and 8, hypothetical pas ratings between 1 and 4 and a column indicating the subject number 7
newdat <- data.frame(cbind('target.frames' = seq(0, 8, by = 0.001), 'pas' = rep(1:4), 'subject' = rep('7')))

newdat$subject <- as.factor(newdat$subject)
newdat$pas <- as.factor(newdat$pas)
newdat$target.frames <- as.numeric(newdat$target.frames)

# Predicting yhats for the hypothetical data points 
newdat$yhat <- predict(pas_m_int, newdata = newdat, type = 'response')

# Plotting
ggplot(newdat) + 
  geom_line(aes(x = target.frames, y = yhat, color = pas)) + 
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  ggtitle('PAS Ratings Model Fits for Participant 7')+
  theme_minimal()
```
    
    iv. Comment on the differences between the fits - mention some advantages and disadvantages of each way  
  The plot illustrating the sigmoid functions have no estimated accuracies below 0.5. This is not the case for the plot illustrating the logitic regression fits showing accuracies below chance for pas 1, 2 and 3. This is an advantage of the sigmoid fit since it is expected that even without perceptual awareness accuracy will be at chance level. However, an advantage of the model fit is that it takes the interaction into account. Another difference is illustrated by the continued increase in accuracy with increase in target frames on the model fit plot. Contrary, on the sigmoid fit the accuracy is kept at a steady level after a rapid increase. This indicates advantages of both the fits. The model fit exhibits the development of accuracy with increase in target frames and the sigmoid fit can visually underline the shift in awareness. 
  
2) Finally, estimate the parameters for all subjects and each of their four PAS ratings. 
```{r 7.2.i}
# Data frame containing x, y, pas and subject
loop_df <- df %>%
  dplyr::select('x' = target.frames, 'y' = correct, pas, subject)

# Empty data frame for output from every loop
output <- data.frame()

# Double loop
for (i in 1:length(unique(df$subject))){
  for (n in 1:length(unique(df$pas))) {
  # Creating a data frame for each participant in each pas  
  subject_df <- loop_df %>% 
    filter(subject == i & pas == n)
  
  # Using the optim function on each subject in each pas
  parameters <- optim(par = c(0.5, 1, 1, 1), 
                     data = subject_df,  
                     fn = RSS, 
                     method = 'L-BFGS-B', 
                     lower = c(0.5, 0.5, -Inf, -Inf), 
                     upper = c(1, 1, Inf, Inf))
  
  # Creating a data frame with subject, pas and each of the four parameters 
  optimated_output <- data.frame(subject = i,
                 pas = n,
                 a = parameters$par[1],
                 b = parameters$par[2],
                 c = parameters$par[3],
                 d = parameters$par[4])
  
  # Row binding the data frames for each subject into the empty data
  output <- rbind(output, optimated_output)
     
  }
}

```

  Then plot the estimated function at the group-level by taking the mean for each of the four parameters, _a_, _b_, _c_ and _d_ across subjects. 

```{r}
# Finding the mean of each of the four parameters grouped by pas
mean_par <- output %>% 
  group_by(pas) %>% 
  summarise(a=mean(a), b=mean(b), c=mean(c), d=mean(d))

# Creating a data frame containing hypothetical target framea values 
plotdf <- data.frame(cbind('x' = seq(0, 8, by = 0.0001)))

# Calculating yhats  using the sigmoid function
yhat_fun <- function(a, b, c, d, x){
  yhat <- a+(b-a)/(1+exp((c-x)/d))
}


plotdf$yhat1 <- yhat_fun(mean_par$a[1], mean_par$b[1], mean_par$c[1], mean_par$d[1], plotdf$x)

plotdf$yhat2 <- yhat_fun(mean_par$a[2], mean_par$b[2], mean_par$c[2], mean_par$d[2], plotdf$x)

plotdf$yhat3 <- yhat_fun(mean_par$a[3], mean_par$b[3], mean_par$c[3], mean_par$d[3], plotdf$x)

plotdf$yhat4 <- yhat_fun(mean_par$a[4], mean_par$b[4], mean_par$c[4], mean_par$d[4], plotdf$x)


ggplot(plotdf) + 
  geom_line(aes(x = x, y = yhat1), color='red') + 
  geom_line(aes(x = x, y = yhat2), color = 'darkgreen') + 
  geom_line(aes(x = x, y = yhat3), color = 'turquoise') + 
  geom_line(aes(x = x, y = yhat4), color = 'violet') + 
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  ggtitle('Estimated group-level mean function')+
  theme_minimal()

```

    i. compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.
  Similar overall tendencies are visible in the two plots. The accuracy in pas 1 is rather constant in both plots, however, it is closer to the expected chance level in figure 5.3.ii.
  One advantage of the psychometric functions may be the possibility to visually examine the shift in awareness. E.g. for pas 3 a shift in awareness is present going from 2 frames to 3. A disadvantage of the estimated group-level mean function is the missing information when moddelling the means.
  